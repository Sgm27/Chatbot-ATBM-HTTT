import os
import asyncio
import streamlit as st
from lightrag import LightRAG, QueryParam
from lightrag.llm.openai import gpt_4o_mini_complete, gpt_4o_complete, openai_embed, gpt_o4_mini_complete
from lightrag.kg.shared_storage import initialize_pipeline_status
from lightrag.utils import setup_logger
from dotenv import load_dotenv

load_dotenv(override=True)

setup_logger("lightrag", level="INFO")

WORKING_DIR = "./data"
if not os.path.exists(WORKING_DIR):
    os.mkdir(WORKING_DIR)

# Initialize session state for chat history
if "messages" not in st.session_state:
    st.session_state.messages = []

async def initialize_rag():
    rag = LightRAG(
        working_dir=WORKING_DIR,
        embedding_func=openai_embed,
        llm_model_func=gpt_o4_mini_complete,
    )
    await rag.initialize_storages()
    await initialize_pipeline_status()
    return rag

async def generate_response(query):
    rag = None
    try:
        rag = await initialize_rag()
        mode = "hybrid"
        full_response = ""
        
        result = await rag.aquery(
            query,
            param=QueryParam(mode=mode, stream=True),
            system_prompt="B·∫°n l√† m·ªôt chuy√™n gia v·ªÅ An to√†n b·∫£o m·∫≠t v√† H·ªá th·ªëng th√¥ng tin. B·∫°n c√≥ th·ªÉ tr·∫£ l·ªùi c√°c c√¢u h·ªèi v·ªÅ An to√†n b·∫£o m·∫≠t v√† H·ªá th·ªëng th√¥ng tin. N·∫øu c√¢u h·ªèi tr·∫Øc nghi·ªám ch·ªâ ch·ªçn 1 ƒë√°p √°n ƒë√∫ng nh·∫•t, gi·∫£i th√≠ch c√°c ƒë√°p √°n sai. Tr·∫£ v·ªÅ c√¢u tr·∫£ l·ªùi d∆∞·ªõi d·∫°ng markdown."
        )
        
        response_placeholder = st.empty()
        async for chunk in result:
            full_response += chunk
            response_placeholder.markdown(full_response + "‚ñå")
        
        response_placeholder.markdown(full_response)
        return full_response
    except Exception as e:
        st.error(f"An error occurred: {e}")
        return f"Error: {str(e)}"
    finally:
        if rag:
            await rag.finalize_storages()

def main():
    st.title("üí¨ ChatBot ATBM HTTT")
    st.subheader("H·ªá th·ªëng tr·∫£ l·ªùi c√°c c√¢u h·ªèi v·ªÅ An to√†n b·∫£o m·∫≠t v√† H·ªá th·ªëng th√¥ng tin")
    
    # Display chat messages from history
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # Input for new message
    if prompt := st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n..."):
        # Add user message to chat history
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        # Display user message in chat container
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # Display assistant response in chat container
        with st.chat_message("assistant"):
            response = asyncio.run(generate_response(prompt))
            st.session_state.messages.append({"role": "assistant", "content": response})

if __name__ == "__main__":
    main() 